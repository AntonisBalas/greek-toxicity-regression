{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#MODELING SCALED OFFENSIVENESS IN GREEK TEXTS THROUGH REGRESSION WITH BEST0WORST SCALING AND PRETRAINED MODELS\n",
        "\n",
        "#National and Kapodistrian University of Athens\n",
        "\n",
        "#Department of Informatics and Telecommunications\n",
        "\n",
        "#Program of Postgraduate Studies: (M.Sc.) in Language Technology\n",
        "\n",
        "#Master's Thesis\n",
        "\n",
        "\n",
        "#Balas Antonis (lt12100021)\n"
      ],
      "metadata": {
        "id": "F1Au5iDot6dS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4O23xLkZqyY",
        "outputId": "ddf37605-dcbb-4fc1-93fd-423c594298bf",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.4)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.11/dist-packages (0.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.11/dist-packages (0.5.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from ekphrasis) (3.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from ekphrasis) (4.67.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from ekphrasis) (0.4.6)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.11/dist-packages (from ekphrasis) (5.10.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from ekphrasis) (3.10.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from ekphrasis) (3.9.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from ekphrasis) (6.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ekphrasis) (2.0.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->ekphrasis) (0.2.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ekphrasis) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ekphrasis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ekphrasis) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ekphrasis) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ekphrasis) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ekphrasis) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ekphrasis) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ekphrasis) (2.9.0.post0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->ekphrasis) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->ekphrasis) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->ekphrasis) (2024.11.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->ekphrasis) (1.17.0)\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `test` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `test`\n"
          ]
        }
      ],
      "source": [
        "# Installing and setting up libraries\n",
        "!pip3 install emoji\n",
        "!pip3 install sentencepiece\n",
        "!pip3 install protobuf\n",
        "!pip3 install torchtext\n",
        "!pip3 install transformers\n",
        "!pip3 install unidecode\n",
        "!pip3 install ekphrasis -U\n",
        "\n",
        "!huggingface-cli login --token   # Login to HuggingFace Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaZTha0haOj4"
      },
      "outputs": [],
      "source": [
        "#Importing all necessary Python libraries\n",
        "\n",
        "import os # Operating system utilities\n",
        "import re # Regular expressions\n",
        "import emoji # For emoji handling\n",
        "import torch # PyTorch for deep learning\n",
        "import json  # JSON parsing\n",
        "import pandas as pd  # DataFrame operations\n",
        "import torch.nn as nn # Neural network modules\n",
        "import numpy as np # Numerical operations\n",
        "import seaborn as sns # Plotting\n",
        "import matplotlib.pyplot as plt # Plotting library\n",
        "from sklearn.model_selection import train_test_split # For splitting dataset\n",
        "from math import sqrt  # Square root function\n",
        "from torch.utils.data import DataLoader, TensorDataset # Data loading utilities\n",
        "from bs4 import BeautifulSoup  # HTML parsing\n",
        "from google.colab import drive # Google Drive integration\n",
        "from transformers import get_linear_schedule_with_warmup  # Learning rate scheduler\n",
        "from torch.utils.data import Dataset # Custom dataset handling\n",
        "from tqdm import tqdm # Progress bar\n",
        "from sklearn.metrics import mean_squared_error # MSE calculation\n",
        "from sklearn.metrics import r2_score # R-squared calculation\n",
        "from transformers import (BertConfig, BertForSequenceClassification, BertTokenizer, AlbertConfig, BertModel,\n",
        "                          AlbertForSequenceClassification, AlbertTokenizer, RobertaConfig,\n",
        "                          RobertaForSequenceClassification, RobertaTokenizer, DebertaConfig,\n",
        "                          DebertaForSequenceClassification, DebertaTokenizer, DebertaV2Config,\n",
        "                          DebertaV2ForSequenceClassification, DebertaV2Tokenizer, XLMRobertaXLConfig,\n",
        "                          XLMRobertaXLForSequenceClassification, AutoTokenizer, AutoConfig,\n",
        "                          AutoModelForSequenceClassification) # Model imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxjxHNjQaJZQ",
        "outputId": "022b828d-4d1c-4295-c9a6-ed19fd75c4ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Mounting Google Drive\n",
        "drive.mount('/content/drive') # Mount Google Drive to access filesi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VWaiLqkbuM5"
      },
      "outputs": [],
      "source": [
        "# Dictionary containing training and optimization hyperparameters\n",
        "args = {\"num_train_epochs\": 20, # Total number of epochs to train the model\n",
        "        'weight_decay': 0.01,  # Weight decay (L2 regularization) to prevent overfitting\n",
        "        'learning_rate': 2e-5, # Learning rate for the optimizer (typically small for transformers)\n",
        "        'adam_epsilon': 1e-2,  # Epsilon for the Adam optimizer to improve numerical stability\n",
        "        'warmup_steps': 0, # Number of warmup steps for the learning rate scheduler\n",
        "        'data_split_ratio': .2,  # Fraction of the data used for validation (20%)\n",
        "        'max_seq_length': 280,  # Maximum input sequence length (token count)\n",
        "        'batch_size': 10, # Number of training examples in each batch\n",
        "        'max_grad_norm': 1.0,   # Maximum norm for gradient clipping to avoid exploding gradients\n",
        "        'patient': 5, # Patience for early stopping (stop if no improvement after 5 epochs)\n",
        "        'delta': 1.0,  # Threshold for considering an improvement in early stopping\n",
        "        \"threads\": 1, # Number of threads to use for data loading or parallel processing\n",
        "        'output_specific_model_dir': \"/content/drive/MyDrive/MODELS/2/Regression/Best_Models/\"} # Configuration dictionary for model-specific settings\n",
        "\n",
        "config = {\"hidden_size\": 5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMEiPCOhc4e6"
      },
      "outputs": [],
      "source": [
        "# Converting emojis in the text to descriptive words (e.g., üòÑ -> smiley face)\n",
        "\n",
        "def emojis_into_text(sentence):\n",
        "    demojized_sent = emoji.demojize(sentence) # Convert emoji to text format like :smile:\n",
        "    emoji_txt = re.sub(r':\\S+:', lambda x: x.group().replace('_', ' ').replace('-', ' ').replace(':', ''),\n",
        "                       demojized_sent)\n",
        "    return emoji_txt\n",
        "\n",
        "# Replacing multiple substrings in a string with a new one\n",
        "\n",
        "def replaceMultiple(main, replacements, new):\n",
        "    for elem in replacements:\n",
        "        if elem in main:\n",
        "            main = main.replace(elem, new)\n",
        "    return main\n",
        "\n",
        "# Normalizes Greek characters\n",
        "\n",
        "def normalize(x):\n",
        "    x = x.replace('Œ¨', 'Œ±')\n",
        "    x = x.replace('Œ≠', 'Œµ')\n",
        "    x = x.replace('ŒÆ', 'Œ∑')\n",
        "    x = replaceMultiple(x, ['ŒØ', 'Œê', 'œä'], 'Œπ')\n",
        "    x = x.replace('œå', 'Œø')\n",
        "    x = replaceMultiple(x, ['œç', 'Œ∞', 'œã'], 'œÖ')\n",
        "    x = x.replace('œé', 'œâ')\n",
        "    return x\n",
        "\n",
        "# Separating digits from words (e.g., 'text12' ‚Üí 'text 12')\n",
        "\n",
        "def sep_digits(x):\n",
        "    return \" \".join(re.split('(\\d+)', x))\n",
        "\n",
        "# Separates punctuation from words by adding spaces around punctuation\n",
        "\n",
        "def sep_punc(x):\n",
        "    punc = '!\"$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ÿõÿåÿüÿõ.¬ª¬´‚Äù'\n",
        "    out = []\n",
        "    for char in x:\n",
        "        if char in punc:\n",
        "            out.append(' ' + char + ' ')\n",
        "        else:\n",
        "            out.append(char)\n",
        "    return \"\".join(out)\n",
        "\n",
        "# Complete preprocessing pipeline for Greek-language social media text\n",
        "\n",
        "def preprocessing_greek(text):\n",
        "    try:  # Remove the UTF-8 BOM (Byte Order Mark)\n",
        "        text = text.decode('utf-8-sig').replace(u'\\ufffd',\n",
        "                                                '?')  # The UTF-8 BOM is a sequence of bytes (EF BB BF) that allows\n",
        "        # the reader to identify a file as being encoded in UTF-8\n",
        "    except:\n",
        "        text = text\n",
        "\n",
        "    soup = BeautifulSoup(text,\n",
        "                         'lxml')  # HTML encoding has not been converted to text, and ended up in text field as\n",
        "\n",
        "    text = soup.get_text() # Remove HTML encoding artifacts\n",
        "\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Replace the RT with whitespace\n",
        "    text = re.sub('rt @\\w+: ', '', text)\n",
        "\n",
        "    # Remove the @user tags\n",
        "    text = re.sub(r'@[a-z0-9_]+', '', text)\n",
        "    text = re.sub(r'username', '', text)\n",
        "\n",
        "    # Remove the url links\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Remove the 'url' and 'html' word\n",
        "    url_words = ['url', 'html', 'http']\n",
        "    for u in url_words:\n",
        "        text = re.sub(u, '', text)\n",
        "\n",
        "    # Convert the emojis into their textual representation\n",
        "    text = emojis_into_text(text)\n",
        "\n",
        "    # Replace '&amp;' with 'Œ∫Œ±Œπ'\n",
        "    text = re.sub(r'&amp;', 'Œ∫Œ±Œπ', text)\n",
        "    text = re.sub(r'&', 'Œ∫Œ±Œπ', text)\n",
        "\n",
        "    # Replace the unicode apostrophe\n",
        "    text = re.sub(r\"‚Äô\", \"'\", text)\n",
        "    text = re.sub(\"‚Äù\", \"'\", text)\n",
        "    text = re.sub(\"‚Äú\", \"'\", text)\n",
        "    text = re.sub(\"'\", '\"', text)\n",
        "\n",
        "    # Remove newlines\n",
        "    text = re.sub(\"\\n\", '', text)\n",
        "    text = text.replace(\"\\\\n\", \"\")\n",
        "\n",
        "    # Normalize characters\n",
        "    text = normalize(text)\n",
        "\n",
        "    # Space out digits and punctuation\n",
        "    text = sep_digits(text)\n",
        "    text = sep_punc(text)\n",
        "\n",
        "    # Mark hashtags\n",
        "    text = re.sub(r'#(\\w+)', r'<hashtag> \\1 </hashtag>', text)\n",
        "\n",
        "    # Remove redundant retweet tokens\n",
        "    text = re.sub(r'rt', ' ', text)\n",
        "\n",
        "    # Remove redundant retweet tokens\n",
        "    text = re.sub(' +', ' ', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Applying preprocessing to an entire corpus and optionally saves it\n",
        "\n",
        "def modify_corpus(data, save=False):\n",
        "    d = {\"Text\": [],\n",
        "         \"Lemma\": [],\n",
        "         \"BWS\": []}\n",
        "\n",
        "    for x in range(len(data)):\n",
        "        d[\"Text\"].append(preprocessing_greek(data[\"Text\"][x]))\n",
        "        d[\"BWS\"].append(data[\"BWS\"][x])\n",
        "        d[\"Lemma\"].append(data[\"Lemma\"][x] if \"Lemma\" in data else None)\n",
        "\n",
        "    edited = pd.DataFrame(d)\n",
        "    edited = edited.drop_duplicates(subset=[\"Text\"])\n",
        "\n",
        "    if save:\n",
        "        edited.to_excel(\"MODIFIED_Corpus-offensive.xlsx\", index=False)\n",
        "\n",
        "    return edited\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3cSIagraLkI"
      },
      "outputs": [],
      "source": [
        "#Dictionary mapping numeric IDs to BERT model variants, including multilingual and Greek-specific models\n",
        "\n",
        "dict_BERT_model_names = {1: 'bert-base-uncased',\n",
        "                         2: 'bert-large-uncased',\n",
        "                         3: 'bert-base-multilingual-uncased',  # multilingual\n",
        "                         4: 'bert-base-multilingual-cased',\n",
        "                         5: 'dimitriz/greek-media-bert-base-uncased',  # Greek-specific\n",
        "                         6: 'nlpaueb/bert-base-greek-uncased-v1'}  # Greek-specific\n",
        "\n",
        "# Dictionary of various AlBERT model variants by size and version\n",
        "dict_AlBERT_model_names = {1: 'albert-base-v1',\n",
        "                           2: 'albert-base-v2',}\n",
        "\n",
        "# Dictionary of RoBERTa model variants (base and large)\n",
        "dict_RoBERTa_model_names = {1: 'roberta-base',\n",
        "                            2: 'roberta-large'}\n",
        "\n",
        "# Dictionary of DeBERTa model variants\n",
        "dict_DeBERTa_model_names = {1: 'microsoft/deberta-base',\n",
        "                            2: 'microsoft/deberta-large',}\n",
        "\n",
        "# Dictionary of  DeBERTaV3 models (includes multilingual variants)\n",
        "dict_DeBERTaV2_model_names = {1: 'microsoft/deberta-v3-large',\n",
        "                              2: 'microsoft/mdeberta-v3-base'}  # multilingual\n",
        "\n",
        "# Dictionary of XLM-RoBERTa multilingual models (base and large)\n",
        "dict_XLM_RoBERTa_model_names = {1: 'xlm-roberta-base',}  # multilingual\n",
        "\n",
        "\n",
        "# Dictionary of miscellaneous multilingual models from various sources\n",
        "dict_multilingual_model_names = {1: 'studio-ousia/mluke-base',  # multilingual\n",
        "                                 2: 'cvcio/comments-el-toxic',  # Greek toxic comment model\n",
        "                                 3: 'autopilot-ai/EthicalEye'}  # multilingual / ethical AI model\n",
        "\n",
        "\n",
        "# Master dictionary that maps each model family to its configuration class,\n",
        "# classification model class, tokenizer class, and name dictionary\n",
        "MODEL_CLASSES = {'BERT': (BertConfig, BertForSequenceClassification, BertTokenizer, dict_BERT_model_names),\n",
        "                 'AlBERT': (AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer, dict_AlBERT_model_names),\n",
        "                 'RoBERTa': (\n",
        "                     RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer, dict_RoBERTa_model_names),\n",
        "                 'DeBERTa': (\n",
        "                     DebertaConfig, DebertaForSequenceClassification, DebertaTokenizer, dict_DeBERTa_model_names),\n",
        "                 'DeBERTaV3': (\n",
        "                     DebertaV2Config, DebertaV2ForSequenceClassification, DebertaV2Tokenizer,\n",
        "                     dict_DeBERTaV2_model_names),\n",
        "                 'XLM_RoBERTa': (XLMRobertaXLConfig, XLMRobertaXLForSequenceClassification, AutoTokenizer,\n",
        "                                 dict_XLM_RoBERTa_model_names),\n",
        "                 'other': (\n",
        "                     AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, dict_multilingual_model_names)}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply full preprocessing pipeline on a sample Greek tweet/text\n",
        "# This includes: lowercasing, emoji conversion, user and URL removal, punctuation spacing,\n",
        "# unicode normalization, hashtag tagging, and spacing digits\n",
        "# Expected result: a cleaned, token-ready version of the input suitable for transformer models\n",
        "\n",
        "preprocessing_greek(\"EŒ§Œ£Œô œÅŒµ Œ±ŒΩŒ±Œ∫Œ±œÑœéœÉœÑœÅŒ± Œ†ŒØœÑŒ∂Œ∑! #NomadsGR\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Wi54JLd0JF7u",
        "outputId": "601c7ef9-4ef3-4557-bd04-5145278a96e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eœÑœÉŒπ œÅŒµ Œ±ŒΩŒ±Œ∫Œ±œÑœâœÉœÑœÅŒ± œÄŒπœÑŒ∂Œ∑ ! <hashtag> nomadsgr </hashtag>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a sample Greek sentence to be tokenized\n",
        "sent = \"EŒ§Œ£Œô œÅŒµ Œ±ŒΩŒ±Œ∫Œ±œÑœéœÉœÑœÅŒ± Œ†ŒØœÑŒ∂Œ∑! #NomadsGR\"\n",
        "\n",
        "# Tokenize the sentence using the tokenizer's encode_plus method\n",
        "encoding = tokenizer.encode_plus(\n",
        "    text=sent,\n",
        "    text_pair=None,                # No second sentence is used\n",
        "    add_special_tokens=True,       # # Adds special tokens like [CLS] and [SEP] required by transformer models\n",
        "    max_length=280,                # Set maximum sequence length to 280 tokens\n",
        "    padding='max_length',          # Pad sequences shorter than max_length with padding tokens\n",
        "    truncation=True,               # Truncate sequences longer than max_length\n",
        "    return_token_type_ids=False,   # Skip token type IDs as they are not used in single-sentence tasks\n",
        "    return_attention_mask=True,    # Generate attention mask to distinguish between real tokens and padding\n",
        "    return_tensors='pt'            # Return results as PyTorch tensors\n",
        ")\n",
        "\n",
        "\n",
        "# Output the encoded representation to inspect input_ids and attention_mask\n",
        "print(encoding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eANlvHeqRTtl",
        "outputId": "55a63d37-7e7a-4584-abd3-ae1b8ca42f16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1041, 29734, 29733, 18199,  1171, 29723,  1155, 16177, 14608,\n",
            "         29726, 14608, 29734, 29739, 29733, 29734, 29732, 14608,  1170, 18199,\n",
            "         29734, 29724, 24824,   999,  1001,  2053, 25666, 28745,  2099,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the length of the input_ids tensor\n",
        "print(len(encoding['input_ids'][0]))\n",
        "\n",
        "# Display the list of token IDs (includes [CLS], [SEP], and padding tokens)\n",
        "encoding['input_ids'][0]"
      ],
      "metadata": {
        "id": "SCYkLMx-U6BU",
        "outputId": "f6737284-21bc-479d-c24d-5a6d42e312fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "280\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  101,  1041, 29734, 29733, 18199,  1171, 29723,  1155, 16177, 14608,\n",
              "        29726, 14608, 29734, 29739, 29733, 29734, 29732, 14608,  1170, 18199,\n",
              "        29734, 29724, 24824,   999,  1001,  2053, 25666, 28745,  2099,   102,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the length of the attension_mask tensor\n",
        "print(len(encoding['attention_mask'][0]))\n",
        "\n",
        "# Display the attention mask tensor\n",
        "# (1 for real tokens, 0 for padding positions)\n",
        "encoding['attention_mask']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xdkGow6UzDO",
        "outputId": "9da63d85-d8b6-404e-a000-674728119b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "280\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_cdy-08aXkn"
      },
      "outputs": [],
      "source": [
        "# Creating a PyTorch DataLoader from a dataframe\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size, shuffle):\n",
        "    data = Data_Preparation(text=df.Text.to_numpy(),\n",
        "                            BWS=df.BWS.to_numpy(),\n",
        "                            tokenizer=tokenizer,\n",
        "                            max_len=max_len)\n",
        "    # Returning a DataLoader object\n",
        "    return DataLoader(data, batch_size=batch_size, pin_memory=False, shuffle=shuffle, num_workers=args[\"threads\"])\n",
        "\n",
        "\n",
        "\n",
        "# Splitting the dataframe into training and validation sets\n",
        "def data_splitting(dataframe, text_column, label_column, split_ratio):\n",
        "    x_train_texts, y_val_texts, x_train_labels, y_val_labels = train_test_split(dataframe[text_column],\n",
        "                                                                                dataframe[label_column],\n",
        "                                                                                random_state=42,\n",
        "                                                                             test_size=split_ratio)\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print('Shape of x_train      : ', x_train_texts.shape)\n",
        "    print('Shape of y_train      : ', x_train_labels.shape)\n",
        "    print('Shape of x_validation : ', y_val_texts.shape)\n",
        "    print('Shape of y_validation : ', y_val_labels.shape)\n",
        "\n",
        "    return x_train_texts, y_val_texts, x_train_labels, y_val_labels\n",
        "\n",
        "\n",
        "# Customing Dataset class for text and label preparation\n",
        "class Data_Preparation(Dataset):\n",
        "\n",
        "    def __init__(self, text, BWS, tokenizer, max_len):\n",
        "        self.text = text\n",
        "        self.label = BWS\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Cleaning up text input\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        # Tokenizing the text\n",
        "        encoding = self.tokenizer.encode_plus(text=text,\n",
        "                                              text_pair=None,\n",
        "                                              add_special_tokens=True,  # Add [CLS] and [SEP]\n",
        "                                              max_length=self.max_len,  # Max length to pad\n",
        "                                              padding='max_length',\n",
        "                                              # Pad sentence according to max length  'max_length'\n",
        "                                              truncation=True,  # Truncate the sentences\n",
        "                                              return_token_type_ids=False,  # Do not return the ids of type tokens\n",
        "                                              return_attention_mask=True,  # Return attention mask\n",
        "                                              return_tensors='pt')  # Return PyTorch tensor\n",
        "\n",
        "         # Returning appropriate format depending on whether label exists\n",
        "        if self.label is None:\n",
        "            return {'input_ids': encoding['input_ids'].flatten(),\n",
        "                    'attention_mask': encoding['attention_mask'].flatten(),\n",
        "                    'text': self.text[index]}\n",
        "\n",
        "\n",
        "        else:\n",
        "            return {'input_ids': encoding['input_ids'].flatten(),\n",
        "                    'attention_mask': encoding['attention_mask'].flatten(),\n",
        "                    'label': torch.tensor(self.label[index], dtype=torch.float),\n",
        "                    'text': self.text[index]}\n",
        "\n",
        " # Combining function to split the data and return either DataFrames or DataLoaders\n",
        "def SplitDataPreparation(dataframe, split_ratio, tokenizer, max_length, batch_size, split_data=True,\n",
        "                         make_dataloaders=True):\n",
        "    if split_data:\n",
        "        train_texts, validation_texts, train_labels, validation_labels = data_splitting(dataframe, 'Text', 'BWS',\n",
        "                                                                                        split_ratio=split_ratio) # Split the dataset into train and validation sets\n",
        "\n",
        "         # Creating pandas DataFrames from the splits\n",
        "        train_df = pd.concat([train_texts, train_labels], axis=1)\n",
        "        validation_df = pd.concat([validation_texts, validation_labels], axis=1)\n",
        "        print(f'Dataset split into train and validation sets using {split_ratio} split ratio.')\n",
        "\n",
        "        if make_dataloaders:\n",
        "             # Converting the dataframes into DataLoader objects\n",
        "            train_dataloader = create_data_loader(df=train_df, tokenizer=tokenizer, max_len=max_length,\n",
        "                                                  batch_size=batch_size, shuffle=False)\n",
        "            val_dataloader = create_data_loader(df=validation_df, tokenizer=tokenizer, max_len=max_length,\n",
        "                                                batch_size=batch_size, shuffle=False)\n",
        "            print('The train and validation dataloaders are ready for training and evaluation.')\n",
        "            return train_dataloader, val_dataloader\n",
        "        else:\n",
        "            return train_df, validation_df\n",
        "\n",
        "    else:\n",
        "        # If not splitting, return only a single training dataloader\n",
        "        train_dataloader = create_data_loader(df=dataframe, tokenizer=tokenizer, max_len=max_length,\n",
        "                                              batch_size=batch_size, shuffle=True)\n",
        "        val_dataloader = None\n",
        "        print(\n",
        "            'The dataframe is not split into train and validation sets. The dataframe converted to train dataloader for training.')\n",
        "        return train_dataloader, val_dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZONUZQAakiQ"
      },
      "outputs": [],
      "source": [
        "# A custom PyTorch module for regression using the [CLS] token from a transformer model\n",
        "\n",
        "class RegressorLastHiddenState(nn.Module):\n",
        "\n",
        "    def __init__(self, pretrained_model, device=\"cpu\", freeze_bert=False):\n",
        "        super(RegressorLastHiddenState, self).__init__()\n",
        "\n",
        "        self.pretrained_model = pretrained_model # Pretrained transformer model\n",
        "        self.regressor = nn.Sigmoid()            # Sigmoid activation for regression output\n",
        "        self.n_input = self.pretrained_model.config.hidden_size  # Hidden size of the model\n",
        "        self.regression = None # Placeholder (not used in this version)\n",
        "        self.device = device # Device: 'cpu' or 'cuda'\n",
        "\n",
        "        # Adding possibility to freeze the BERT model to avoid fine-tuning BERT params\n",
        "        if freeze_bert:\n",
        "            for param in self.pretrained_model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "         # Getting the outputs from the pretrained model\n",
        "        outputs = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
        "\n",
        "        # Extracting the last hidden state of the token `[CLS]` for regression task\n",
        "        last_hidden_state = outputs[0]\n",
        "        cls_embeddings = last_hidden_state[:, 0]\n",
        "\n",
        "        # Feeding input to regressor to get regression output\n",
        "        regression_output = self.regressor(cls_embeddings)\n",
        "\n",
        "        # regression_output = my_activation(regression_output)\n",
        "\n",
        "         # Output shapes:\n",
        "        # last_hidden_state       : (batch_size, sequence_length, hidden_size)\n",
        "        # cls_embeddings          : (batch_size, hidden_size)\n",
        "        # regression_output       : (batch_size, 1)\n",
        "\n",
        "        return regression_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8_VZ0i1azYo"
      },
      "outputs": [],
      "source": [
        "#Training & Evaluation Process\n",
        "\n",
        "def setup_pretrained_model(classificationModel, model_name, args, train_dataloader, device=\"cpu\"):\n",
        "    # Loading the pretrained model from the specified name\n",
        "    pretrained_model = classificationModel.from_pretrained(\n",
        "        model_name,\n",
        "        output_attentions=False,  # Whether the model returns attentions weights\n",
        "        output_hidden_states=True) # Output hidden states (needed for extracting [CLS] token)\n",
        "\n",
        "    # Wrap the pretrained model in a regression head\n",
        "    model = RegressorLastHiddenState(pretrained_model=pretrained_model, device=device)\n",
        "    # Move the model to the specified device (CPU or GPU)\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    # Calculating the total number of training steps\n",
        "    num_train_steps = int(len(train_dataloader) * args[\"num_train_epochs\"])\n",
        "\n",
        "    # Preparing parameters for the optimizer with weight decay configuration\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "\n",
        "    # Separating parameters into two groups for weight decay vs no weight decay\n",
        "    optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "                                     'weight_decay': args['weight_decay']},\n",
        "                                    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "                                     'weight_decay': 0.0}]\n",
        "\n",
        "    # Defining the AdamW optimizer with custom learning rate and epsilon\n",
        "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'],\n",
        "                                  betas=(0.9, 0.999))\n",
        "\n",
        "    # Defining a linear learning rate scheduler with warmup\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args['warmup_steps'],\n",
        "                                                num_training_steps=num_train_steps)\n",
        "\n",
        "    return model, optimizer, scheduler, num_train_steps\n",
        "\n",
        "\n",
        "\n",
        "def splitData(tokenizer_name, model_name, dataframe, split_ratio, max_length, batch_size, split_data, make_dataloaders):\n",
        "    # Loading tokenizer from pretrained model\n",
        "    tokenizer = tokenizer_name.from_pretrained(model_name)\n",
        "    # Splitting the dataset and prepare DataLoader objects\n",
        "    train_dataloader, val_dataloader = SplitDataPreparation(dataframe=dataframe,\n",
        "                                                            split_ratio=split_ratio,\n",
        "                                                            tokenizer=tokenizer,\n",
        "                                                            max_length=max_length,\n",
        "                                                            batch_size=batch_size,\n",
        "                                                            split_data=split_data,\n",
        "                                                            make_dataloaders=make_dataloaders)\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "\n",
        "def train_the_model(model, train_dataloader, val_dataloader, optimizer, scheduler, args, patience, model_class, model_filename):\n",
        "\n",
        "    # Defining the loss function (Huber loss is more robust to outliers)\n",
        "    def loss_function(delta, device=\"cuda\"):\n",
        "      return nn.HuberLoss(delta=delta).to(device)\n",
        "\n",
        "    training_stats = []    # Stores overall training statistics\n",
        "    best_score = None      # Best validation loss seen so far\n",
        "    counter = 0            # Early stopping counter\n",
        "    lossF = loss_function(args[\"delta\"])\n",
        "    statistics_per_epoch = dict()  # Stores detailed epoch-wise predictions\n",
        "\n",
        "    # Training loop across epochs\n",
        "    for epoch in range(args['num_train_epochs']):\n",
        "        print(f'\\n======== EPOCH {epoch + 1} / {args[\"num_train_epochs\"]} ========\\n')\n",
        "        print('TRAINING MODEL...')\n",
        "\n",
        "        training_loss = 0\n",
        "        r_squared = 0\n",
        "        training_mse = 0\n",
        "        training_rmse = 0\n",
        "        labels = 0\n",
        "        stats = {'Text': [],\n",
        "                 'BWS': [],\n",
        "                 'Prediction': [],\n",
        "                 'Training': []}\n",
        "\n",
        "        model.train()   # Setting model to training mode\n",
        "\n",
        "        for step, batch in enumerate(tqdm(train_dataloader, desc='Training iteration')):\n",
        "\n",
        "            # Progressing update every 661 batches  482  349  927  727 457\n",
        "            if (step+1) % 1000 == 0:\n",
        "                # Reporting progress\n",
        "                print(f' Batch {step+1} of {len(train_dataloader)}')\n",
        "\n",
        "            # Moving  input data to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_masks = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            # Zero the gradients/Resetting gradients before backward pass\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Performing forward propagation/ Computing the predictions\n",
        "            predictions = model(input_ids=input_ids, attention_mask=attention_masks)\n",
        "\n",
        "            # Computing the loss\n",
        "            loss = lossF(predictions, labels)\n",
        "\n",
        "            # Detaching predictions and move labels to CPU for logging\n",
        "            predictions = predictions.detach().cpu().numpy()\n",
        "            labels = labels.to('cpu').numpy()\n",
        "\n",
        "            # Logging predictions\n",
        "            for text, prediction, label in zip(batch[\"text\"], predictions, labels):\n",
        "              stats['Text'].append(text)\n",
        "              stats['BWS'].append(float(label))\n",
        "              stats['Prediction'].append(float(prediction))\n",
        "              stats['Training'].append(True)\n",
        "\n",
        "            # Backward pass: compute gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Cliping the gradient value to avoid exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
        "                                           max_norm=args['max_grad_norm'])  # Avoid exploding of gradients\n",
        "\n",
        "            # Using optimizer to take gradient step and Update the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Resetting gradients after update\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Accumulating total training loss\n",
        "            training_loss += loss.item()\n",
        "\n",
        "            # Clearing cache to save memory\n",
        "            torch.cuda.empty_cache()\n",
        "            del input_ids, attention_masks\n",
        "\n",
        "        # Calculating average loss for this epoch\n",
        "        train_loss_of_epoch = training_loss / len(train_dataloader)\n",
        "\n",
        "        print(f'Train Loss: {train_loss_of_epoch:.3f}')\n",
        "\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "        print('\\nEVALUATING MODEL...')\n",
        "\n",
        "        model.eval() # Setting the model to evaluation mode\n",
        "\n",
        "        eval_loss = 0\n",
        "        eval_mse = 0\n",
        "        eval_rmse = 0\n",
        "        eval_r_squared = 0\n",
        "        output = 0\n",
        "\n",
        "        # Calculation of gradient not required during evaluation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for batch in tqdm(val_dataloader, desc='Evaluation iteration'):\n",
        "                # Moving input tensors to the designated device (GPU or CPU)\n",
        "                ids_inputs = batch['input_ids'].to(device)\n",
        "                att_masks = batch['attention_mask'].to(device)\n",
        "                targets = batch['label'].to(device)\n",
        "\n",
        "                # Performing forward propagation/ Getting model predictions\n",
        "                outputs = model(input_ids=ids_inputs, attention_mask=att_masks)\n",
        "\n",
        "                # Computing the loss\n",
        "                logits = lossF(outputs, targets)\n",
        "\n",
        "                # Moving logits and labels to CPU\n",
        "                outputs = outputs.detach().cpu().numpy()\n",
        "                targets = targets.to('cpu').numpy()\n",
        "\n",
        "                # Saving predictions and corresponding true values for analysis\n",
        "                for text, prediction, label in zip(batch[\"text\"], outputs, targets):\n",
        "                  stats['Text'].append(text)\n",
        "                  stats['BWS'].append(float(label))\n",
        "                  stats['Prediction'].append(float(prediction))\n",
        "                  stats['Training'].append(False)\n",
        "\n",
        "                # Accumulating total validation loss\n",
        "                eval_loss += logits.item()\n",
        "\n",
        "                # Free up memory by deleting unused tensors\n",
        "                torch.cuda.empty_cache()\n",
        "                del ids_inputs, att_masks, outputs\n",
        "\n",
        "        # Calculating average validation loss for the current epoch\n",
        "        loss_of_epoch_val = eval_loss / len(val_dataloader)\n",
        "\n",
        "        print(f'Validation Loss: {loss_of_epoch_val:.3f}')\n",
        "\n",
        "        # Saving epoch statistics (predictions, labels, texts)\n",
        "        statistics_per_epoch[f\"{epoch+1}\"] = stats\n",
        "\n",
        "        # Saving best model checkpoint (based on lowest validation loss)\n",
        "        if best_score is None:\n",
        "            best_score = loss_of_epoch_val\n",
        "            save_model(model, model_filename, f\"{args['output_specific_model_dir']}{model_class}/{re.sub('/', '-', model_filename)}\")\n",
        "        else:\n",
        "            # Checking if val_loss improves or not.\n",
        "            if loss_of_epoch_val < best_score:\n",
        "                # val_loss improving then updating the latest best_score and saving the current model\n",
        "                best_score = loss_of_epoch_val\n",
        "                save_model(model, model_filename, f\"{args['output_specific_model_dir']}{model_class}/{re.sub('/', '-', model_filename)}\")\n",
        "            else:\n",
        "                # val_loss does not improve then increase the counter, stop training if it exceeds the amount of patience\n",
        "                counter += 1\n",
        "                if counter >= patience:\n",
        "                    print('EARLY STOPPING!')\n",
        "                    break\n",
        "\n",
        "    print('Training and evaluation process complete!')\n",
        "\n",
        "    return statistics_per_epoch    # Returning epoch-wise detailed prediction statistics\n",
        "\n",
        "\n",
        "def run_it(model_class, model_name, args, total_stat):\n",
        "    # Preparing the training and validation dataloaders using the tokenizer for the given model\n",
        "    train_dataloader, val_dataloader = splitData(MODEL_CLASSES[model_class][2], model_name, training_dataset,\n",
        "                                                 args['data_split_ratio'], args['max_seq_length'], args['batch_size'],\n",
        "                                                 True, True) # split data into train/val # create DataLoaders\n",
        "\n",
        "    # Loading the pretrained model and prepare optimizer, scheduler, etc.\n",
        "    model, optimizer, scheduler, num_train_steps = setup_pretrained_model(MODEL_CLASSES[model_class][1], model_name,\n",
        "                                                                          args, train_dataloader, device)\n",
        "\n",
        "    # Training and evaluating the model, returning stats for each epoch\n",
        "    statistics = train_the_model(model, train_dataloader, val_dataloader, optimizer, scheduler, args,\n",
        "                                     args[\"patient\"], model_class, model_name)\n",
        "\n",
        "    # Preparing directory for saving statistics\n",
        "    s_model_name = re.sub(\"/\", \"-\", model_name)\n",
        "    save_to = f\"{args['output_specific_model_dir']}{model_class}/{s_model_name}\"\n",
        "    if not os.path.exists(save_to):\n",
        "        os.makedirs(save_to)\n",
        "\n",
        "    # Saving per-epoch statistics to a JSON file\n",
        "    file_path = f\"{save_to}/statistics_per_epoch.json\"\n",
        "    with open(file_path, \"w\") as json_file:\n",
        "        json.dump(statistics, json_file)\n",
        "        print(f\"Best statistics saved as {file_path}\")\n",
        "\n",
        "    # Cleaning up GPU memory\n",
        "    torch.cuda.empty_cache()\n",
        "    del model, optimizer, scheduler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjCwWVYyepQo"
      },
      "outputs": [],
      "source": [
        "# Function to save the model\n",
        "def save_model(model, experiment_name, model_output_dir):\n",
        "    # Creating the output directory if it doesn't exist\n",
        "    if not os.path.exists(model_output_dir):\n",
        "        os.makedirs(model_output_dir)\n",
        "\n",
        "    # Replacing slashes in experiment name to avoid directory issues\n",
        "    experiment_name=re.sub(r'/', '-', experiment_name)\n",
        "\n",
        "    # Defining full output path for the model file\n",
        "    output_model_file = os.path.join(model_output_dir, experiment_name)\n",
        "\n",
        "    # Getting the actual model if it's wrapped in a parallel/distributed wrapper\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "\n",
        "    # Save the model's state_dict (recommended way)\n",
        "    torch.save(model_to_save.state_dict(), f\"{output_model_file}.pt\")\n",
        "\n",
        "    # Optionally save the entire model object (less portable but easier to reload)\n",
        "    torch.save(model_to_save, f\"{output_model_file}.pt\")\n",
        "\n",
        "    print(f'Model saved to {model_output_dir} as {experiment_name}')\n",
        "\n",
        "\n",
        "# Function to load a previously saved model\n",
        "def load_saved_model(defined_model, experiment_name, model_output_dir):\n",
        "    # Building the full path to the saved model\n",
        "    saved_model_path = os.path.join(model_output_dir, experiment_name)\n",
        "    loaded_model = None\n",
        "\n",
        "    # Loading state_dict if file ends in '.pth'\n",
        "    if experiment_name.endswith('.pth'):\n",
        "        loaded_model = defined_model.load_state_dict(torch.load(saved_model_path))\n",
        "        print('Model loaded successfully')\n",
        "\n",
        "    # Loading entire model object if file ends in '.pt'\n",
        "    if experiment_name.endswith('.pt'):\n",
        "        loaded_model = torch.load(saved_model_path)\n",
        "        print('Model loaded successfully')\n",
        "    else:\n",
        "        print('No such model found.')\n",
        "    return loaded_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uvyG6UWeSZx"
      },
      "outputs": [],
      "source": [
        "print('TRAINING PARAMETERS:\\n')\n",
        "print('Loss function used: HuberLoss')\n",
        "print('Optimizer used: AdamW')\n",
        "print(f\"Learning Rate: {args['learning_rate']:.5}\")\n",
        "print(f\"Adam Epsilon: {args['adam_epsilon']}\")\n",
        "print(f'Betas: {0.9, 0.999}')\n",
        "print(f'Weight Decay: {args[\"weight_decay\"]}')\n",
        "print(f'Batch Size: {args[\"batch_size\"]}')\n",
        "print(f'Number of training epochs: {args[\"num_train_epochs\"]}')\n",
        "print(f'Maximum Sequence Length: {args[\"max_seq_length\"]}')\n",
        "print(f'Warm-up Steps: {args[\"warmup_steps\"]}')\n",
        "print(f\"Delta value for Huber Loss: {args['delta']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p23c6TEOnMWH"
      },
      "outputs": [],
      "source": [
        "# Define the device to run the model on: GPU if available, otherwise CPU\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Loading the dataset from an Excel file\n",
        "# data = pd.read_excel(\n",
        "\n",
        "\n",
        "# Apply preprocessing and formatting to the dataset without saving it to a file\n",
        "#     \"/content/drive/MyDrive/MODELS/corpus/fiction.xlsx\")\n",
        "# training_dataset = modify_corpus(data, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPgx9Hbegm-Y"
      },
      "outputs": [],
      "source": [
        "# total_stat = dict()\n",
        "\n",
        "# for model_class in MODEL_CLASSES:\n",
        "#   if model_class in [\"BERT\"]:\n",
        "#     total_stat[model_class] = dict()\n",
        "#     for model_name in MODEL_CLASSES[model_class][3].values():\n",
        "#       if model_name in [\"nlpaueb/bert-base-greek-uncased-v1\",\n",
        "#                         'dimitriz/greek-media-bert-base-uncased']:\n",
        "#         run_it(model_class, model_name, args, total_stat)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining paths to all available corpora for training\n",
        "corpuses = {\n",
        "    'tweets':         \"/content/drive/MyDrive/MODELS/corpus/tweets.xlsx\",\n",
        "    'fiction':          \"/content/drive/MyDrive/MODELS/corpus/fiction.xlsx\",\n",
        "    'blogs':          \"/content/drive/MyDrive/MODELS/corpus/blogs.xlsx\",\n",
        "    'fiction+blogs':  \"/content/drive/MyDrive/MODELS/corpus/fiction+blogs.xlsx\",\n",
        "    'tweets+fiction': \"/content/drive/MyDrive/MODELS/corpus/tweets+fiction.xlsx\",\n",
        "    'tweets+blogs':   \"/content/drive/MyDrive/MODELS/corpus/tweets+blogs.xlsx\",\n",
        "    'all':            \"/content/drive/MyDrive/MODELS/corpus/fiction+blogs+tweets.xlsx\"\n",
        "    }\n",
        "\n",
        "# Selecting device based on GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Looping over each corpus configuration\n",
        "for folder_name, corpus_path in corpuses.items():\n",
        "  # Loading data from Excel file\n",
        "  data = pd.read_excel(corpus_path)\n",
        "  # Apply text preprocessing\n",
        "  training_dataset = modify_corpus(data, False)\n",
        "\n",
        "  # Setting output directory specific to this corpus\n",
        "  args['output_specific_model_dir'] = os.path.join(\n",
        "      \"/content/drive/MyDrive/MODELS/Regression/Best_Models/\", folder_name)\n",
        "  args['output_specific_model_dir'] = args['output_specific_model_dir'] + '/'\n",
        "  # Dictionary to hold training statistics per model\n",
        "  total_stat = dict()\n",
        "  # Iterate through each model class\n",
        "  for model_class in MODEL_CLASSES:\n",
        "    if model_class in [\"BERT\"]:\n",
        "      total_stat[model_class] = dict()\n",
        "                  # Looping through all BERT model names (pretrained)\n",
        "      for model_name in MODEL_CLASSES[model_class][3].values():\n",
        "        # Running the full fine-tuning and evaluation pipeline\n",
        "        run_it(model_class, model_name, args, total_stat)\n"
      ],
      "metadata": {
        "id": "4GqPjwQ1kcoG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}